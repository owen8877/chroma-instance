@article{Gulrajani2017,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
eprint = {1704.00028},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1704.00028.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {5768--5778},
title = {{Improved training of wasserstein GANs}},
volume = {2017-December},
year = {2017}
}
@article{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network features a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification database to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Furthermore, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
doi = {10.1145/2897824.2925974},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/2897824.2925974.pdf:pdf},
isbn = {9781450342797},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Colorization,Convolutional neural network},
number = {4},
pages = {1--11},
title = {{Let there be color!: Joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification}},
volume = {35},
year = {2016}
}
@article{Guadarrama2017,
abstract = {We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a "Visual Turing Test".},
archivePrefix = {arXiv},
arxivId = {1705.07208},
author = {Guadarrama, Sergio and Dahl, Ryan and Bieber, David and Norouzi, Mohammad and Shlens, Jonathon and Murphy, Kevin},
eprint = {1705.07208},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1705.07208.pdf:pdf},
journal = {arXiv},
month = {may},
pages = {1--17},
title = {{PixColor: Pixel Recursive Colorization}},
url = {http://arxiv.org/abs/1705.07208},
year = {2017}
}
@article{Zhao2018,
abstract = {While many image colorization algorithms have recently shown the capability of producing plausible color versions from gray-scale photographs, they still suffer from the problems of context confusion and edge color bleeding. To address context confusion, we propose to incorporate the pixel-level object semantics to guide the image colorization. The rationale is that human beings perceive and distinguish colors based on the object's semantic categories. We propose a hierarchical neural network with two branches. One branch learns what the object is while the other branch learns the object's colors. The network jointly optimizes a semantic segmentation loss and a colorization loss. To attack edge color bleeding we generate more continuous color maps with sharp edges by adopting a joint bilateral upsamping layer at inference. Our network is trained on PASCAL VOC2012 and COCO-stuff with semantic segmentation labels and it produces more realistic and finer results compared to the colorization state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1808.01597},
author = {Zhao, Jiaojiao and Liu, Li and Snoek, Cees G.M. and Han, Jungong and Shao, Ling},
eprint = {1808.01597},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/0236.pdf:pdf},
journal = {arXiv},
pages = {1--12},
title = {{Pixel-level semantics guided image colorization}},
year = {2018}
}
@article{Vondrick2018,
abstract = {We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.},
archivePrefix = {arXiv},
arxivId = {1806.09594},
author = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
doi = {10.1007/978-3-030-01261-8_24},
eprint = {1806.09594},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Carl_Vondrick_Self-supervised_Tracking_by_ECCV_2018_paper.pdf:pdf},
isbn = {9783030012601},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Colorization,Self-supervised learning,Tracking,Video},
pages = {402--419},
title = {{Tracking emerges by colorizing videos}},
volume = {11217 LNCS},
year = {2018}
}
@article{Ci2018,
abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.03240v2},
author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
doi = {10.1145/3240508.3240661},
eprint = {arXiv:1808.03240v2},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1808.03240.pdf:pdf},
isbn = {9781450356657},
journal = {MM 2018 - Proceedings of the 2018 ACM Multimedia Conference},
keywords = {Edit Propagation,GANs,Interactive Colorization},
pages = {1536--1544},
title = {{User-guided deep anime line art colorization with conditional adversarial networks}},
year = {2018}
}
@article{Bahng2018,
abstract = {This paper proposes a novel approach to generate multiple color palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated color palette. In contrast to existing approaches, our model can understand rich text, whether it is a single word, a phrase, or a sentence, and generate multiple possible palettes from it. For this task, we introduce our manually curated dataset called Palette-and-Text (PAT). Our proposed model called Text2Colors consists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks. The former captures the semantics of the text input and produce relevant color palettes. The latter colorizes a grayscale image using the generated color palette. Our evaluation results show that people preferred our generated palettes over ground truth palettes and that our model can effectively reflect the given palette when colorizing an image.},
archivePrefix = {arXiv},
arxivId = {1804.04128},
author = {Bahng, Hyojin and Yoo, Seungjoo and Cho, Wonwoong and Park, David Keetae and Wu, Ziming and Ma, Xiaojuan and Choo, Jaegul},
doi = {10.1007/978-3-030-01258-8_27},
eprint = {1804.04128},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Hyojin_Bahng_Coloring_with_Words_ECCV_2018_paper.pdf:pdf},
isbn = {9783030012571},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {443--459},
title = {{Coloring with words: Guiding image colorization through text-based palette generation}},
volume = {11216 LNCS},
year = {2018}
}
@article{Iizuka2019,
abstract = {The remastering of vintage film comprises of a diversity of sub-tasks including super-resolution, noise removal, and contrast enhancement which aim to restore the deteriorated film medium to its original state. Additionally, due to the technical limitations of the time, most vintage film is either recorded in black and white, or has low quality colors, for which colorization becomes necessary. In this work, we propose a single framework to tackle the entire remastering task semi-interactively. Our work is based on temporal convolutional neural networks with attention mechanisms trained on videos with data-driven deterioration simulation. Our proposed source-reference attention allows the model to handle an arbitrary number of reference color images to colorize long videos without the need for segmentation while maintaining temporal consistency. Quantitative analysis shows that our framework outperforms existing approaches, and that, in contrast to existing approaches, the performance of our framework increases with longer videos and more reference color images.},
archivePrefix = {arXiv},
arxivId = {arXiv:2009.08692v1},
author = {Iizuka, Satoshi and Simo-Serra, Edgar},
doi = {10.1145/3355089.3356570},
eprint = {arXiv:2009.08692v1},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/2009.08692.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Colorization,Convolutional network,Remastering,Restoration,Source-reference attention},
number = {6},
pages = {1--13},
title = {{DeepRemaster: Temporal source-reference attention networks for comprehensive video enhancement}},
volume = {38},
year = {2019}
}
@article{Yoo2019,
abstract = {Despite recent advancements in deep learning-based automatic colorization, they are still limited when it comes to few-shot learning. Existing models require a significant amount of training data. To tackle this issue, we present a novel memory-augmented colorization model MemoPainter that can produce high-quality colorization with limited data. In particular, our model is able to capture rare instances and successfully colorize them. Also, we propose a novel threshold triplet loss that enables unsupervised training of memory networks without the need for class labels. Experiments show that our model has superior quality in both few-shot and one-shot colorization tasks.},
author = {Yoo, Seungjoo and Bahng, Hyojin and Chung, Sunghyo and Lee, Junsoo and Chang, Jaehyuk and Choo, Jaegul},
doi = {10.1109/CVPR.2019.01154},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Yoo_Coloring_With_Limited_Data_Few-Shot_Colorization_via_Memory_Augmented_Networks_CVPR_2019_paper.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Computational Photography,Computer Vision Theory,Deep Learning,Image and Video Synthesis,Vi,Vision + Graphics},
pages = {11275--11284},
title = {{Coloring with limited data: Few-shot colorization via memory augmented networks}},
volume = {2019-June},
year = {2019}
}
@article{Lee2020,
abstract = {This paper tackles the automatic colorization task of a sketch image given an already-colored reference image. Colorizing a sketch image is in high demand in comics, animation, and other content creation applications, but it suffers from information scarcity of a sketch image. To address this, a reference image can render the colorization process in a reliable and user-driven manner. However, it is difficult to prepare for a training data set that has a sufficient amount of semantically meaningful pairs of images as well as the ground truth for a colored image reflecting a given reference (e.g., coloring a sketch of an originally blue car given a reference green car). To tackle this challenge, we propose to utilize the identical image with geometric distortion as a virtual reference, which makes it possible to secure the ground truth for a colored output image. Furthermore, it naturally provides the ground truth for dense semantic correspondence, which we utilize in our internal attention mechanism for color transfer from reference to sketch input. We demonstrate the effectiveness of our approach in various types of sketch image colorization via quantitative as well as qualitative evaluation against existing methods.},
archivePrefix = {arXiv},
arxivId = {2005.05207},
author = {Lee, Junsoo and Kim, Eungyeup and Lee, Yunsung and Kim, Dongjun and Chang, Jaehyuk and Choo, Jaegul},
doi = {10.1109/CVPR42600.2020.00584},
eprint = {2005.05207},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Lee_Reference-Based_Sketch_Image_Colorization_Using_Augmented-Self_Reference_and_Dense_Semantic_CVPR_2020_paper.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {5800--5809},
title = {{Reference-based sketch image colorization using augmented-self reference and dense semantic correspondence}},
year = {2020}
}
@article{Vitoria2020,
abstract = {The colorization of grayscale images is an ill-posed problem, with multiple correct solutions. In this paper, we propose an adversarial learning colorization approach coupled with semantic information. A generative network is used to infer the chromaticity of a given grayscale image conditioned to semantic clues. This network is framed in an adversarial model that learns to colorize by incorporating perceptual and semantic understanding of color and class distributions. The model is trained via a fully self-supervised strategy. Qualitative and quantitative results show the capacity of the proposed method to colorize images in a realistic way achieving state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1907.09837},
author = {Vitoria, Patricia and Raad, Lara and Ballester, Coloma},
doi = {10.1109/WACV45572.2020.9093389},
eprint = {1907.09837},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Vitoria_ChromaGAN_Adversarial_Picture_Colorization_with_Semantic_Class_Distribution_WACV_2020_paper.pdf:pdf},
isbn = {9781728165530},
journal = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
pages = {2434--2443},
title = {{ChromaGAN: Adversarial picture colorization with semantic class distribution}},
year = {2020}
}
@article{Su2020,
abstract = {Image colorization is inherently an ill-posed problem with multi-modal uncertainty. Previous methods leverage the deep neural network to map input grayscale images to plausible color outputs directly. Although these learning-based methods have shown impressive performance, they usually fail on the input images that contain multiple objects. The leading cause is that existing models perform learning and colorization on the entire image. In the absence of a clear figure-ground separation, these models cannot effectively locate and learn meaningful object-level semantics. In this paper, we propose a method for achieving instance-aware colorization. Our network architecture leverages an off-the-shelf object detector to obtain cropped object images and uses an instance colorization network to extract object-level features. We use a similar network to extract the full-image features and apply a fusion module to full object-level and image-level features to predict the final colors. Both colorization networks and fusion modules are learned from a large-scale dataset. Experimental results show that our work outperforms existing methods on different quality metrics and achieves state-of-the-art performance on image colorization.},
author = {Su, Jheng Wei and Chu, Hung Kuo and Huang, Jia Bin},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/Su_Instance-Aware_Image_Colorization_CVPR_2020_paper.pdf:pdf},
journal = {arXiv},
title = {{Instance-aware Image Colorization}},
year = {2020}
}
@article{Anwar2020,
abstract = {Image colorization is an essential image processing and computer vision branch to colorize images and videos. Recently, deep learning techniques progressed notably for image colorization. This article presents a comprehensive survey of recent state-of-the-art colorization using deep learning algorithms, describing their fundamental block architectures in terms of skip connections, input etc. as well as optimizers, loss functions, training protocols, and training data etc. Generally, we can roughly categorize the existing colorization techniques into seven classes. Besides, we also provide some additional essential issues, such as benchmark datasets and evaluation metrics. We also introduce a new dataset specific to colorization and perform an experimental evaluation of the publicly available methods. In the last section, we discuss the limitations, possible solutions, and future research directions of the rapidly evolving topic of deep image colorization that the community should further address. Dataset and Codes for evaluation will be publicly available at https://github.com/saeed-anwar/ColorSurvey},
archivePrefix = {arXiv},
arxivId = {2008.10774},
author = {Anwar, Saeed and Tahir, Muhammad and Li, Chongyi and Mian, Ajmal and Khan, Fahad Shahbaz and Muzaffar, Abdul Wahab},
eprint = {2008.10774},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/2008.10774.pdf:pdf},
month = {aug},
pages = {1--19},
title = {{Image Colorization: A Survey and Dataset}},
url = {http://arxiv.org/abs/2008.10774},
year = {2020}
}
@article{Zhang2018,
abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called 'perceptual losses'? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
archivePrefix = {arXiv},
arxivId = {1801.03924},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
doi = {10.1109/CVPR.2018.00068},
eprint = {1801.03924},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1801.03924.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {586--595},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
year = {2018}
}
@article{Cheng2015,
abstract = {This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.},
archivePrefix = {arXiv},
arxivId = {1605.00075},
author = {Cheng, Zezhou and Yang, Qingxiong and Sheng, Bin},
doi = {10.1109/ICCV.2015.55},
eprint = {1605.00075},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1605.00075.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {415--423},
title = {{Deep colorization}},
volume = {2015 Inter},
year = {2015}
}
@article{Carlucci2018,
author = {Carlucci, Fabio Maria and Russo, Paolo and Caputo, Barbara},
doi = {10.1109/LRA.2018.2812225},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/08306886.pdf:pdf},
issn = {2377-3766},
journal = {IEEE Robotics and Automation Letters},
month = {jul},
number = {3},
pages = {2386--2393},
publisher = {IEEE},
title = {{(DE)$^2$CO: Deep Depth Colorization}},
url = {http://ieeexplore.ieee.org/document/8306886/},
volume = {3},
year = {2018}
}
@article{Zhang2016,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
archivePrefix = {arXiv},
arxivId = {1603.08511},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
doi = {10.1007/978-3-319-46487-9_40},
eprint = {1603.08511},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1603.08511.pdf:pdf},
isbn = {9783319464862},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {CNNs,Colorization,Self-supervised learning,Vision for graphics},
pages = {649--666},
title = {{Colorful image colorization}},
volume = {9907 LNCS},
year = {2016}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
doi = {10.1007/978-3-319-46493-0_35},
eprint = {1603.06668},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1603.06668.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {577--593},
title = {{Learning representations for automatic colorization}},
volume = {9908 LNCS},
year = {2016}
}
@article{Royer2017,
abstract = {We develop a probabilistic technique for colorizing grayscale natural images. In light of the intrinsic uncertainty of this task, the proposed probabilistic framework has numerous desirable properties. In particular, our model is able to produce multiple plausible and vivid colorizations for a given grayscale image and is one of the first colorization models to provide a proper stochastic sampling scheme. Moreover, our training procedure is supported by a rigorous theoretical framework that does not require any ad hoc heuristics and allows for efficient modeling and learning of the joint pixel color distribution. We demonstrate strong quantitative and qualitative experimental results on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset.},
archivePrefix = {arXiv},
arxivId = {1705.04258},
author = {Royer, Amelie and Kolesnikov, Alexander and Lampert, Christoph H.},
eprint = {1705.04258},
file = {:home/xdroid/Documents/CSE392 ML/project/papers/1705.04258.pdf:pdf},
journal = {arXiv},
pages = {1--15},
title = {{Probabilistic image colorization}},
year = {2017}
}
